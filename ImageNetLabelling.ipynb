{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping image net to 16 named classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from shutil import rmtree, copy2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "IMAGENET_DIR = \"/kaggle/input/imagenetmini-1000/imagenet-mini/train\" \n",
    "OUTPUT_DIR = \"/kaggle/working/output_data\"\n",
    "OUTPUT_ZIP = \"/kaggle/working/output.zip\"\n",
    "CATEGORY_MAPPINGS = {\n",
    "    \"knife\": ['n03041632'],\n",
    "    \"keyboard\": ['n03085013', 'n04505470'],\n",
    "    \"elephant\": ['n02504013', 'n02504458'],\n",
    "    \"bicycle\": ['n02835271', 'n03792782'],\n",
    "    \"airplane\": ['n02690373', 'n03955296', 'n13861050', 'n13941806'],\n",
    "    \"clock\": ['n02708093', 'n03196217', 'n04548280'],\n",
    "    \"oven\": ['n03259401', 'n04111414', 'n04111531'],\n",
    "    \"chair\": ['n02791124', 'n03376595', 'n04099969', 'n00605023', 'n04429376'],\n",
    "    \"bear\": ['n02132136', 'n02133161', 'n02134084', 'n02134418'],\n",
    "    \"boat\": ['n02951358', 'n03344393', 'n03662601', 'n04273569', 'n04612373', 'n04612504'],\n",
    "    \"cat\": [\"n02122878\", \"n02123045\", \"n02123159\", \"n02126465\", \"n02123394\", \"n02123597\", \"n02124075\", \"n02125311\"],\n",
    "    \"bottle\": ['n02823428', 'n03937543', 'n03983396', 'n04557648', 'n04560804', 'n04579145', 'n04591713'],\n",
    "    \"truck\": ['n03345487', 'n03417042', 'n03770679', 'n03796401', 'n00319176', 'n01016201', 'n03930630', 'n03930777', \n",
    "              'n05061003', 'n06547832', 'n10432053', 'n03977966', 'n04461696', 'n04467665'],\n",
    "    \"car\": ['n02814533', 'n03100240', 'n03100346', 'n13419325', 'n04285008'],\n",
    "    \"bird\": ['n01321123', 'n01514859', 'n01792640', 'n07646067', 'n01530575', 'n01531178', 'n01532829', 'n01534433', \n",
    "             'n01537544', 'n01558993', 'n01562265', 'n01560419', 'n01582220', 'n10281276', 'n01592084', 'n01601694', \n",
    "             'n01614925', 'n01616318', 'n01622779', 'n01795545', 'n01796340', 'n01797886', 'n01798484', 'n01817953', \n",
    "             'n01818515', 'n01819313', 'n01820546', 'n01824575', 'n01828970', 'n01829413', 'n01833805', 'n01843065', \n",
    "             'n01843383', 'n01855032', 'n01855672', 'n07646821', 'n01860187', 'n02002556', 'n02002724', 'n02006656', \n",
    "             'n02007558', 'n02009229', 'n02009912', 'n02011460', 'n02013706', 'n02017213', 'n02018207', 'n02018795', \n",
    "             'n02025239', 'n02027492', 'n02028035', 'n02033041', 'n02037110', 'n02051845', 'n02056570'],\n",
    "    \"dog\": ['n02085782', 'n02085936', 'n02086079', 'n02086240', 'n02086646', 'n02086910', 'n02087046', 'n02087394', \n",
    "            'n02088094', 'n02088238', 'n02088364', 'n02088466', 'n02088632', 'n02089078', 'n02089867', 'n02089973', \n",
    "            'n02090379', 'n02090622', 'n02090721', 'n02091032', 'n02091134', 'n02091244', 'n02091467', 'n02091635', \n",
    "            'n02091831', 'n02092002', 'n02092339', 'n02093256', 'n02093428', 'n02093647', 'n02093754', 'n02093859', \n",
    "            'n02093991', 'n02094114', 'n02094258', 'n02094433', 'n02095314', 'n02095570', 'n02095889', 'n02096051', \n",
    "            'n02096294', 'n02096437', 'n02096585', 'n02097047', 'n02097130', 'n02097209', 'n02097298', 'n02097474', \n",
    "            'n02097658', 'n02098105', 'n02098286', 'n02099267', 'n02099429', 'n02099601', 'n02099712', 'n02099849', \n",
    "            'n02100236', 'n02100583', 'n02100735', 'n02100877', 'n02101006', 'n02101388', 'n02101556', 'n02102040', \n",
    "            'n02102177', 'n02102318', 'n02102480', 'n02102973', 'n02104029', 'n02104365', 'n02105056', 'n02105162', \n",
    "            'n02105251', 'n02105505', 'n02105641', 'n02105855', 'n02106030', 'n02106166', 'n02106382', 'n02106550', \n",
    "            'n02106662', 'n02107142', 'n02107312', 'n02107574', 'n02107683', 'n02107908', 'n02108000', 'n02108422', \n",
    "            'n02108551', 'n02108915', 'n02109047', 'n02109525', 'n02109961', 'n02110063', 'n02110185', 'n02110627', \n",
    "            'n02110806', 'n02110958', 'n02111129', 'n02111277', 'n08825211', 'n02111500', 'n02112018', 'n02112350', \n",
    "            'n02112706', 'n02113023', 'n02113624', 'n02113712', 'n02113799', 'n02113978']\n",
    "}\n",
    "\n",
    "def filter_and_split_imagenet_data(src_dir, dest_dir, category_mappings, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Copy and split ImageNet data into train and test sets for specific categories.\n",
    "    \"\"\"\n",
    "    if os.path.exists(dest_dir):\n",
    "        rmtree(dest_dir)  \n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    \n",
    "    for category, wnids in category_mappings.items():\n",
    "        train_dir = os.path.join(dest_dir, \"train\", category)\n",
    "        test_dir = os.path.join(dest_dir, \"test\", category)\n",
    "        os.makedirs(train_dir, exist_ok=True)\n",
    "        os.makedirs(test_dir, exist_ok=True)\n",
    "        \n",
    "        all_files = []\n",
    "        for wnid in wnids:\n",
    "            wnid_dir = os.path.join(src_dir, wnid)\n",
    "            if os.path.exists(wnid_dir):\n",
    "                all_files.extend([os.path.join(wnid_dir, f) for f in os.listdir(wnid_dir) if os.path.isfile(os.path.join(wnid_dir, f))])\n",
    "        \n",
    "        train_files, test_files = train_test_split(all_files, test_size=test_size)\n",
    "        \n",
    "        for i, file_path in enumerate(train_files):\n",
    "            new_file_name = f\"{category.upper()}{i + 1}.JPEG\"\n",
    "            copy2(file_path, os.path.join(train_dir, new_file_name))\n",
    "        for i, file_path in enumerate(test_files):\n",
    "            new_file_name = f\"{category.upper()}{i + 1}.JPEG\"\n",
    "            copy2(file_path, os.path.join(test_dir, new_file_name))\n",
    "\n",
    "def create_zip_from_directory(source_dir, output_zip_path):\n",
    "    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        total_files = sum([len(files) for _, _, files in os.walk(source_dir)])\n",
    "        with tqdm(total=total_files, desc=\"Zipping files\", unit=\"file\") as pbar:\n",
    "            for root, _, files in os.walk(source_dir):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, start=source_dir)\n",
    "                    zipf.write(file_path, arcname)\n",
    "                    pbar.update(1)\n",
    "\n",
    "filter_and_split_imagenet_data(IMAGENET_DIR, OUTPUT_DIR, CATEGORY_MAPPINGS, test_size=0.2)\n",
    "create_zip_from_directory(OUTPUT_DIR, OUTPUT_ZIP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "input_dir = \"/kaggle/working/output_data\"\n",
    "output_zip = \"/kaggle/working/cannydata.zip\"\n",
    "num_samples = 1500\n",
    "train_ratio = 0.8\n",
    "image_files_by_class = defaultdict(list)\n",
    "\n",
    "for subset in ['train', 'test']:\n",
    "    subset_dir = os.path.join(input_dir, subset)\n",
    "    for root, dirs, files in os.walk(subset_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                class_name = os.path.basename(root)  \n",
    "                image_files_by_class[class_name].append(os.path.join(root, file))\n",
    "\n",
    "sampled_files = []\n",
    "\n",
    "for class_name, class_files in image_files_by_class.items():\n",
    "    if len(class_files) < num_samples // len(image_files_by_class):\n",
    "        sampled_files.extend(class_files)\n",
    "    else:\n",
    "        sampled_files.extend(random.sample(class_files, num_samples // len(image_files_by_class)))\n",
    "        \n",
    "train_files, test_files = train_test_split(sampled_files, train_size=train_ratio, random_state=42)\n",
    "\n",
    "def apply_canny(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        return None\n",
    "    edges = cv2.Canny(image, 100, 200)\n",
    "    return edges\n",
    "    \n",
    "added_files = set()\n",
    "\n",
    "with zipfile.ZipFile(output_zip, 'w') as zipf:\n",
    "    for subset, files in zip(['train', 'test'], [train_files, test_files]):\n",
    "        for file_path in files:\n",
    "            class_name = os.path.basename(os.path.dirname(file_path))\n",
    "            filename = os.path.basename(file_path)\n",
    "            processed_filename = f\"{subset}/{class_name}/{filename}\"\n",
    "\n",
    "            if processed_filename not in added_files:\n",
    "                edges = apply_canny(file_path)\n",
    "                if edges is not None:\n",
    "                    temp_dir = os.path.dirname(f\"/tmp/{processed_filename}\")\n",
    "                    os.makedirs(temp_dir, exist_ok=True)\n",
    "                    temp_path = f\"/tmp/{processed_filename}\"\n",
    "                    cv2.imwrite(temp_path, edges)\n",
    "                    zipf.write(temp_path, arcname=processed_filename)\n",
    "                    added_files.add(processed_filename)\n",
    "\n",
    "print(f\"Processed images saved to {output_zip}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "input_dir = '/kaggle/input/cue-conflict'\n",
    "output_dir = '/kaggle/working'\n",
    "temp_dir = os.path.join(output_dir, 'folder')  \n",
    "test_split_ratio = 0.2  \n",
    "\n",
    "train_dir = os.path.join(temp_dir, 'train')\n",
    "test_dir = os.path.join(temp_dir, 'test')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    if not files:\n",
    "        continue  \n",
    "    \n",
    "    relative_path = os.path.relpath(root, input_dir)\n",
    "    train_subdir = os.path.join(train_dir, relative_path)\n",
    "    test_subdir = os.path.join(test_dir, relative_path)\n",
    "    os.makedirs(train_subdir, exist_ok=True)\n",
    "    os.makedirs(test_subdir, exist_ok=True)\n",
    "\n",
    "    random.shuffle(files)\n",
    "    split_idx = int(len(files) * (1 - test_split_ratio))\n",
    "    train_files = files[:split_idx]\n",
    "    test_files = files[split_idx:]\n",
    "\n",
    "    for file in train_files:\n",
    "        shutil.copy2(os.path.join(root, file), os.path.join(train_subdir, file))\n",
    "    for file in test_files:\n",
    "        shutil.copy2(os.path.join(root, file), os.path.join(test_subdir, file))\n",
    "\n",
    "zip_path = os.path.join(output_dir, 'dataset.zip')\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(temp_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(file_path, temp_dir) \n",
    "            zipf.write(file_path, arcname)\n",
    "\n",
    "print(f\"Dataset saved at: {zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
