{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10279246,"sourceType":"datasetVersion","datasetId":6360696},{"sourceId":10285912,"sourceType":"datasetVersion","datasetId":6365329},{"sourceId":10288219,"sourceType":"datasetVersion","datasetId":6367036}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split\n\ndef create_data_loaders(dataset_path, batch_size, val_split=0.2, seed=42):\n    torch.manual_seed(seed)\n\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)), \n        transforms.ToTensor(),        \n        # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        # transforms.Resize((256, 256))\n    ])\n    \n    dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n    val_size = int(len(dataset) * val_split)\n    train_size = len(dataset) - val_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    print(f\"Train size: {train_size}, Validation size: {val_size}\")\n    print(f\"Class names: {dataset.classes}\")\n    \n    return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:06:40.636739Z","iopub.execute_input":"2024-12-25T19:06:40.637321Z","iopub.status.idle":"2024-12-25T19:06:43.540886Z","shell.execute_reply.started":"2024-12-25T19:06:40.637281Z","shell.execute_reply":"2024-12-25T19:06:43.540159Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ndef load_fine_tuned_model(model_name, model_path, num_classes=16):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    if model_name == \"VGG11\":\n        model = VGG11(num_classes=num_classes, pretrained=False)\n    elif model_name == \"VGG16\":\n        model = VGG16Model(num_classes=num_classes, pretrained=False)\n    elif model_name == \"ResNet50\":\n        model = ResNet50Model(num_classes=num_classes, pretrained=False)\n    elif model_name == \"DenseNet121\":\n        model = DenseNet121Model(num_classes=num_classes, pretrained=False)\n    elif model_name == \"ViT\":\n        model = ViTModel(num_classes=num_classes, pretrained=False)\n    elif model_name == \"ViTsmall\":\n        model = SmallViTModel(num_classes=num_classes, pretrained=False)    \n    else:\n        raise ValueError(f\"Unsupported model name: {model_name}\")\n\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model = model.to(device)\n    model.eval()\n    print(f'{model_name} model loaded and ready for evaluation.')\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:06:47.007484Z","iopub.execute_input":"2024-12-25T19:06:47.007926Z","iopub.status.idle":"2024-12-25T19:06:48.455863Z","shell.execute_reply.started":"2024-12-25T19:06:47.007897Z","shell.execute_reply":"2024-12-25T19:06:48.455165Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"**Teacher Models**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torchvision.models import vit_b_16\n\nclass ViTModel(nn.Module): \n    def __init__(self, num_classes, pretrained=True):\n        super(ViTModel, self).__init__()\n        self.model = vit_b_16(pretrained=pretrained)\n        \n        for param in self.model.parameters():\n            param.requires_grad = False\n            \n        in_features = self.model.heads.head.in_features\n        self.model.heads.head = nn.Linear(in_features, num_classes)\n        for param in self.model.heads.head.parameters():\n            param.requires_grad = True\n\n    def forward(self, x):\n        return self.model(x)\n\n    def get_model(self):\n        return self.model\n\n# num_classes = 10 \n# vit_model = ViTModel(num_classes=num_classes, pretrained=True)\n# model_instance = vit_model.get_model()\n# print(model_instance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:06:51.746942Z","iopub.execute_input":"2024-12-25T19:06:51.747356Z","iopub.status.idle":"2024-12-25T19:06:51.752882Z","shell.execute_reply.started":"2024-12-25T19:06:51.747319Z","shell.execute_reply":"2024-12-25T19:06:51.752004Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nimport torch.nn as nn\n\nclass VGG16Model(nn.Module):\n    def __init__(self, num_classes, pretrained=True):\n        super(VGG16Model, self).__init__()\n        self.model = models.vgg16(pretrained=pretrained)\n\n        for param in self.model.features.parameters():\n            param.requires_grad = False\n        in_features = self.model.classifier[6].in_features\n        self.model.classifier[6] = nn.Linear(in_features, num_classes)\n        for param in self.model.classifier[6].parameters():\n            param.requires_grad = True\n\n    def forward(self, x):\n        return self.model(x)\n        \n    def get_model(self):\n        return self.model\n\n\n# num_classes = 10  \n# vgg_model = VGG16Model(num_classes=num_classes, pretrained=True)\n# model_instance = vgg_model.get_model()\n# print(model_instance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:06:54.529916Z","iopub.execute_input":"2024-12-25T19:06:54.530233Z","iopub.status.idle":"2024-12-25T19:06:54.536050Z","shell.execute_reply.started":"2024-12-25T19:06:54.530203Z","shell.execute_reply":"2024-12-25T19:06:54.535038Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"**Student Model**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport timm\n\nclass SmallViTModel(nn.Module):\n    def __init__(self, num_classes, pretrained=False):\n        super(SmallViTModel, self).__init__()\n        \n        # Create a ViT model using a variant that works for 224x224 images\n        # Ensure the model matches the number of classes used in the teacher models\n        self.model = timm.create_model('vit_small_patch16_224', pretrained=pretrained)\n\n        # Freeze backbone parameters\n        for param in self.model.parameters():\n            param.requires_grad = True\n\n        # Modify the classifier to match num_classes (ensure it's consistent with the teacher model)\n        in_features = self.model.head.in_features\n        self.model.reset_classifier(num_classes=num_classes)\n        \n        # Ensure the new classifier is trainable\n        for param in self.model.head.parameters():\n            param.requires_grad = True\n\n    def forward(self, x):\n        return self.model(x)\n\n    def get_model(self):\n        return self.model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:06:57.610109Z","iopub.execute_input":"2024-12-25T19:06:57.610394Z","iopub.status.idle":"2024-12-25T19:07:00.592830Z","shell.execute_reply.started":"2024-12-25T19:06:57.610372Z","shell.execute_reply":"2024-12-25T19:07:00.591642Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"**Fine Tuning**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport os\nimport zipfile\nfrom tqdm import tqdm\nfrom torchvision import datasets, transforms\n\ndef fine_tune_model(model_class, dataset_path, output_path, num_classes, epochs=10, batch_size=32, learning_rate=0.001):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    train_loader, val_loader = create_data_loaders(dataset_path, batch_size)\n    model = model_class(num_classes=num_classes, pretrained=True)\n    # model = model_class(num_classes=num_classes, pretrained=False)\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n\n    # Optimizer configuration for ViT and ResNet\n    if isinstance(model, ViTModel):  # For ViT\n        optimizer = optim.Adam(model.get_model().heads.head.parameters(), lr=learning_rate)\n    elif isinstance(model, SmallViTModel):  # For SmallViTModel\n        optimizer = optim.Adam(model.get_model().get_classifier().parameters(), lr=learning_rate)\n    # elif isinstance(model, ResNet50Model):  # For ResNet\n    #     optimizer = optim.Adam(model.get_model().fc.parameters(), lr=learning_rate)  # Use 'fc' for ResNet\n    else:  # For other models (e.g., DenseNet, etc.)\n        optimizer = optim.Adam(model.get_model().classifier.parameters(), lr=learning_rate)\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        total_samples = 0\n\n        print(f\"Epoch {epoch+1}/{epochs}: Training started...\")\n        for images, labels in tqdm(train_loader, desc=\"Training\", unit=\"batch\"):\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            train_correct += torch.sum(preds == labels)\n            total_samples += images.size(0)\n\n        train_loss_avg = train_loss / total_samples\n        train_accuracy = (train_correct.double() / total_samples) * 100\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        with torch.no_grad():\n            print(\"Validation started...\")\n            for images, labels in tqdm(val_loader, desc=\"Validating\", unit=\"batch\"):\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item() * images.size(0)\n                _, preds = torch.max(outputs, 1)\n                val_correct += torch.sum(preds == labels)\n\n        val_loss_avg = val_loss / len(val_loader.dataset)\n        val_accuracy = (val_correct.double() / len(val_loader.dataset)) * 100\n        print(f\"Epoch {epoch+1}/{epochs}: \"\n              f\"Training Accuracy: {train_accuracy:.4f}, \"\n              f\"Training Loss: {train_loss_avg:.4f} | \"\n              f\"Validation Accuracy: {val_accuracy:.4f}, \"\n              f\"Validation Loss: {val_loss_avg:.4f}\")\n\n    model_name = model_class.__name__.lower()\n    model_path = os.path.join(output_path, f\"{model_name}_fine_tuned_{num_classes}_classes.pth\")\n    torch.save(model.state_dict(), model_path)\n\n    # torch.save(model.get_model().state_dict(), model_path)\n    print(f\"Model saved at {model_path}\")\n    \n    zip_path = model_path.replace(\".pth\", \".zip\")\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        zipf.write(model_path, os.path.basename(model_path))\n    # os.remove(model_path)\n    print(f\"Model saved and zipped to {zip_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:07:03.182651Z","iopub.execute_input":"2024-12-25T19:07:03.182979Z","iopub.status.idle":"2024-12-25T19:07:03.194345Z","shell.execute_reply.started":"2024-12-25T19:07:03.182954Z","shell.execute_reply":"2024-12-25T19:07:03.193337Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/labellledimagenet/train\" \noutput_path = \"/kaggle/working/FinetunedModels\"\nos.makedirs(output_path, exist_ok=True)\nmodel_class = SmallViTModel\nfine_tune_model(model_class, dataset_path, output_path, num_classes=16, epochs=2, batch_size=128, learning_rate=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T12:50:17.068778Z","iopub.execute_input":"2024-12-25T12:50:17.069146Z","iopub.status.idle":"2024-12-25T12:51:29.910696Z","shell.execute_reply.started":"2024-12-25T12:50:17.069115Z","shell.execute_reply":"2024-12-25T12:51:29.909443Z"}},"outputs":[{"name":"stdout","text":"Train size: 4819, Validation size: 1204\nClass names: ['airplane', 'bear', 'bicycle', 'bird', 'boat', 'bottle', 'car', 'cat', 'chair', 'clock', 'dog', 'elephant', 'keyboard', 'knife', 'oven', 'truck']\nEpoch 1/2: Training started...\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 38/38 [00:27<00:00,  1.36batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation started...\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 10/10 [00:04<00:00,  2.13batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/2: Training Accuracy: 16.9122, Training Loss: 2.5553 | Validation Accuracy: 34.8837, Validation Loss: 2.2592\nEpoch 2/2: Training started...\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 38/38 [00:27<00:00,  1.36batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation started...\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 10/10 [00:04<00:00,  2.02batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/2: Training Accuracy: 44.2415, Training Loss: 2.0526 | Validation Accuracy: 49.4186, Validation Loss: 1.8675\nModel saved at /kaggle/working/FinetunedModels/smallvitmodel_fine_tuned_16_classes.pth\nModel saved and zipped to /kaggle/working/FinetunedModels/smallvitmodel_fine_tuned_16_classes.zip\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/labellledimagenet/train\" \noutput_path = \"/kaggle/working/FinetunedModelsCC\"  \nos.makedirs(output_path, exist_ok=True)\nmodel_class = VGG16Model  \nfine_tune_model(model_class, dataset_path, output_path, num_classes=16, epochs=1, batch_size=128, learning_rate=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:07:17.420172Z","iopub.execute_input":"2024-12-25T19:07:17.420451Z","iopub.status.idle":"2024-12-25T19:08:31.382993Z","shell.execute_reply.started":"2024-12-25T19:07:17.420429Z","shell.execute_reply":"2024-12-25T19:08:31.382066Z"}},"outputs":[{"name":"stdout","text":"Train size: 4819, Validation size: 1204\nClass names: ['airplane', 'bear', 'bicycle', 'bird', 'boat', 'bottle', 'car', 'cat', 'chair', 'clock', 'dog', 'elephant', 'keyboard', 'knife', 'oven', 'truck']\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 233MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1: Training started...\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 38/38 [00:28<00:00,  1.33batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation started...\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 10/10 [00:07<00:00,  1.27batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1: Training Accuracy: 84.3121, Training Loss: 0.5777 | Validation Accuracy: 97.0930, Validation Loss: 0.1115\nModel saved at /kaggle/working/FinetunedModelsCC/vgg16model_fine_tuned_16_classes.pth\nModel saved and zipped to /kaggle/working/FinetunedModelsCC/vgg16model_fine_tuned_16_classes.zip\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/labellledimagenet/train\"   \noutput_path = \"/kaggle/working/FinetunedModels\"\n\nos.makedirs(output_path, exist_ok=True)\nmodel_class = ViTModel \nfine_tune_model(model_class, dataset_path, output_path, num_classes=16, epochs=1, batch_size=128, learning_rate=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:08:47.474209Z","iopub.execute_input":"2024-12-25T19:08:47.474529Z","iopub.status.idle":"2024-12-25T19:09:46.666180Z","shell.execute_reply.started":"2024-12-25T19:08:47.474506Z","shell.execute_reply":"2024-12-25T19:09:46.665268Z"}},"outputs":[{"name":"stdout","text":"Train size: 4819, Validation size: 1204\nClass names: ['airplane', 'bear', 'bicycle', 'bird', 'boat', 'bottle', 'car', 'cat', 'chair', 'clock', 'dog', 'elephant', 'keyboard', 'knife', 'oven', 'truck']\nEpoch 1/1: Training started...\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 38/38 [00:30<00:00,  1.24batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation started...\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 10/10 [00:08<00:00,  1.20batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1: Training Accuracy: 12.3055, Training Loss: 2.7227 | Validation Accuracy: 21.6777, Validation Loss: 2.6096\nModel saved at /kaggle/working/FinetunedModels/vitmodel_fine_tuned_16_classes.pth\nModel saved and zipped to /kaggle/working/FinetunedModels/vitmodel_fine_tuned_16_classes.zip\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/cue-conflict-splitdata/train\" \noutput_path = \"/kaggle/working/FinetunedModels\"\nos.makedirs(output_path, exist_ok=True)\nmodel_class = ViTModel \nfine_tune_model(model_class, dataset_path, output_path, num_classes=16, epochs=5, batch_size=128, learning_rate=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:58:51.597097Z","iopub.execute_input":"2024-12-25T17:58:51.597399Z","iopub.status.idle":"2024-12-25T17:59:51.236703Z","shell.execute_reply.started":"2024-12-25T17:58:51.597374Z","shell.execute_reply":"2024-12-25T17:59:51.235606Z"}},"outputs":[{"name":"stdout","text":"Train size: 705, Validation size: 176\nClass names: ['airplane', 'bear', 'bicycle', 'bird', 'boat', 'bottle', 'car', 'cat', 'chair', 'clock', 'dog', 'elephant', 'keyboard', 'knife', 'oven', 'truck']\nEpoch 1/5: Training started...\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 6/6 [00:06<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Validation started...\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 2/2 [00:02<00:00,  1.43s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: Training Accuracy: 5.6738, Training Loss: 2.8552 | Validation Accuracy: 4.5455, Validation Loss: 2.8262\nEpoch 2/5: Training started...\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 6/6 [00:05<00:00,  1.15batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation started...\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 2/2 [00:01<00:00,  1.08batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: Training Accuracy: 7.8014, Training Loss: 2.7790 | Validation Accuracy: 8.5227, Validation Loss: 2.7688\nEpoch 3/5: Training started...\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 6/6 [00:05<00:00,  1.15batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation started...\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 2/2 [00:01<00:00,  1.07batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: Training Accuracy: 11.9149, Training Loss: 2.7083 | Validation Accuracy: 11.9318, Validation Loss: 2.7136\nEpoch 4/5: Training started...\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 6/6 [00:05<00:00,  1.13batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation started...\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 2/2 [00:01<00:00,  1.06batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: Training Accuracy: 15.8865, Training Loss: 2.6396 | Validation Accuracy: 18.7500, Validation Loss: 2.6602\nEpoch 5/5: Training started...\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 6/6 [00:05<00:00,  1.14batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation started...\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 2/2 [00:01<00:00,  1.08batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: Training Accuracy: 19.5745, Training Loss: 2.5731 | Validation Accuracy: 22.7273, Validation Loss: 2.6087\nModel saved at /kaggle/working/FinetunedModels/vitmodel_fine_tuned_16_classes.pth\nModel saved and zipped to /kaggle/working/FinetunedModels/vitmodel_fine_tuned_16_classes.zip\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"**Shape Biased Student Tuning**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport os\nimport zipfile\nfrom tqdm import tqdm\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split, ConcatDataset\n\ndef create_curriculum_data_loaders(dataset_path_original, dataset_path_shape, batch_size, val_split=0.2, seed=42, curriculum_stage=1):\n    \"\"\"\n    Create data loaders for curriculum learning.\n    - Stage 1: Only shape-biased data.\n    - Stage 2: Mix of shape-biased and original data.\n    \"\"\"\n    torch.manual_seed(seed)\n\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    dataset_shape = datasets.ImageFolder(root=dataset_path_shape, transform=transform)\n    dataset_original = datasets.ImageFolder(root=dataset_path_original, transform=transform)\n\n    if curriculum_stage == 1:\n        combined_dataset = dataset_shape\n    else:\n        combined_dataset = ConcatDataset([dataset_shape, dataset_original])\n\n    val_size = int(len(combined_dataset) * val_split)\n    train_size = len(combined_dataset) - val_size\n    train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    print(f\"Train size: {train_size}, Validation size: {val_size}\")\n    print(f\"Shape Class Names: {dataset_shape.classes}\")\n    if curriculum_stage > 1:\n        print(f\"Original Class Names: {dataset_original.classes}\")\n\n    return train_loader, val_loader\n\ndef fine_tune_model_curriculum(model_class, dataset_path_original, dataset_path_shape, output_path, num_classes, epochs=10, batch_size=32, learning_rate=0.001):\n    \"\"\"\n    Fine-tune the model using curriculum learning.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    curriculum_epochs = [int(epochs * 0.5), epochs]  \n\n    model = model_class(num_classes=num_classes, pretrained=True)\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.get_model().heads.head.parameters(), lr=learning_rate)\n\n    for epoch in range(epochs):\n        curriculum_stage = 1 if epoch < curriculum_epochs[0] else 2\n        train_loader, val_loader = create_curriculum_data_loaders(\n            dataset_path_original, dataset_path_shape, batch_size, curriculum_stage=curriculum_stage\n        )\n\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        total_samples = 0\n\n        print(f\"Epoch {epoch + 1}/{epochs}: Training started (Curriculum Stage {curriculum_stage})...\")\n        for images, labels in tqdm(train_loader, desc=\"Training\", unit=\"batch\"):\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            train_correct += torch.sum(preds == labels)\n            total_samples += images.size(0)\n\n        train_loss_avg = train_loss / total_samples\n        train_accuracy = (train_correct.double() / total_samples) * 100\n\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        with torch.no_grad():\n            print(\"Validation started...\")\n            for images, labels in tqdm(val_loader, desc=\"Validating\", unit=\"batch\"):\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item() * images.size(0)\n                _, preds = torch.max(outputs, 1)\n                val_correct += torch.sum(preds == labels)\n\n        val_loss_avg = val_loss / len(val_loader.dataset)\n        val_accuracy = (val_correct.double() / len(val_loader.dataset)) * 100\n        print(f\"Epoch {epoch + 1}/{epochs}: \"\n              f\"Training Accuracy: {train_accuracy:.4f}, \"\n              f\"Training Loss: {train_loss_avg:.4f} | \"\n              f\"Validation Accuracy: {val_accuracy:.4f}, \"\n              f\"Validation Loss: {val_loss_avg:.4f}\")\n\n    model_name = model_class.__name__.lower()\n    model_path = os.path.join(output_path, f\"{model_name}_fine_tuned_{num_classes}_classes.pth\")\n    torch.save(model.state_dict(), model_path)\n\n    zip_path = model_path.replace(\".pth\", \".zip\")\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        zipf.write(model_path, os.path.basename(model_path))\n\n    print(f\"Model saved and zipped to {zip_path}\")\n\ndataset_path_original = \"/kaggle/input/labellledimagenet/train\"\ndataset_path_shape = \"/kaggle/input/cue-conflict-splitdata/train\"\noutput_path = \"/kaggle/working/FinetunedModels\"\nos.makedirs(output_path, exist_ok=True)\nmodel_class = ViTModel  \nfine_tune_model_curriculum(model_class, dataset_path_original, dataset_path_shape, output_path, num_classes=16, epochs=5, batch_size=128, learning_rate=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:11:15.522958Z","iopub.execute_input":"2024-12-25T19:11:15.523251Z","iopub.status.idle":"2024-12-25T19:11:21.081738Z","shell.execute_reply.started":"2024-12-25T19:11:15.523230Z","shell.execute_reply":"2024-12-25T19:11:21.080520Z"}},"outputs":[{"name":"stdout","text":"Train size: 820, Validation size: 204\nShape Class Names: ['airplane', 'bear', 'bicycle', 'bird', 'boat', 'bottle', 'car', 'cat', 'chair', 'clock', 'dog', 'elephant', 'keyboard', 'knife', 'oven', 'truck']\nEpoch 1/5: Training started (Curriculum Stage 1)...\n","output_type":"stream"},{"name":"stderr","text":"Training:  29%|██▊       | 2/7 [00:03<00:09,  1.91s/batch]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-6566af8988ce>\u001b[0m in \u001b[0;36m<cell line: 129>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mViTModel\u001b[0m  \u001b[0;31m# Replace with your actual ViT model class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0mfine_tune_model_curriculum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_path_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_path_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-6566af8988ce>\u001b[0m in \u001b[0;36mfine_tune_model_curriculum\u001b[0;34m(model_class, dataset_path_original, dataset_path_shape, output_path, num_classes, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mtrain_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":11},{"cell_type":"markdown","source":"**Basic LM**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport zipfile\nimport os\n\ndef logit_matching_loss(student_logits, teacher_logits, student_temperature=1.0, teacher_temperature=1.0):\n    student_logits = student_logits - student_logits.mean(dim=-1, keepdim=True)\n    teacher_logits = teacher_logits - teacher_logits.mean(dim=-1, keepdim=True)\n    student_logits = student_logits / student_temperature\n    teacher_logits = teacher_logits / teacher_temperature\n    loss = F.mse_loss(student_logits, teacher_logits)\n    return loss\n\ndef distill(model_class, teacher_model, teacher_model_name, train_loader, val_loader, num_classes, \n            epochs=10, batch_size=32, learning_rate=0.001, \n            student_temperature=2.0, teacher_temperature=2.0, alpha=0.5, save_dir=\"/kaggle/working/DistilledModels\"):\n\n    student_model = model_class(num_classes=num_classes)\n    optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    student_model = student_model.to(device)\n    teacher_model = teacher_model.to(device)\n    teacher_model.eval()  \n    criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(epochs):\n        student_model.train()\n        running_loss = 0.0\n\n        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n            for images, labels in pbar:\n                images, labels = images.to(device), labels.to(device)\n                student_outputs = student_model(images)\n                with torch.no_grad():\n                    teacher_outputs = teacher_model(images)\n                ce_loss = criterion(student_outputs, labels)\n                distillation_loss = logit_matching_loss(student_outputs, teacher_outputs, \n                                                        student_temperature=student_temperature,\n                                                        teacher_temperature=teacher_temperature)\n\n                total_loss = (1 - alpha) * ce_loss + alpha * distillation_loss\n                optimizer.zero_grad()\n                total_loss.backward()\n                optimizer.step()\n                running_loss += total_loss.item()\n\n                pbar.set_postfix(loss=running_loss / len(pbar))  \n\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n        student_model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = student_model(images)\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        accuracy = 100 * correct / total\n        print(f\"Validation Accuracy at epoch {epoch+1}: {accuracy:.2f}%\")\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    model_filename = f\"{teacher_model_name}_distilled_model.pth\" \n    model_path = os.path.join(save_dir, model_filename)\n    torch.save(student_model.state_dict(), model_path)\n    print(f\"Distilled model saved to {model_path}\")\n\n    zip_filename = model_filename.replace(\".pth\", \".zip\")\n    zip_path = os.path.join(save_dir, zip_filename)\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        zipf.write(model_path, os.path.basename(model_path))\n        \n    print(f\"Distilled model zip file saved to {zip_path}\")\n    return student_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:11:33.848030Z","iopub.execute_input":"2024-12-25T19:11:33.848354Z","iopub.status.idle":"2024-12-25T19:11:33.859877Z","shell.execute_reply.started":"2024-12-25T19:11:33.848329Z","shell.execute_reply":"2024-12-25T19:11:33.858913Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/labellledimagenet/train\"\nbatch_size = 128\n\ntrain_loader, val_loader = create_data_loaders(dataset_path, batch_size)\nteacher_model_name = \"VGG16\"  \nteacher_model_path = \"/kaggle/working/FinetunedModelsCC/vgg16model_fine_tuned_16_classes.pth\"  \nteacher_model = load_fine_tuned_model(teacher_model_name, teacher_model_path, num_classes=16)\nstudent_model_class = SmallViTModel\n\ndistilled_model = distill(\n    model_class=student_model_class, \n    teacher_model=teacher_model, \n    teacher_model_name=teacher_model_name,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    num_classes=16,\n    epochs=8,\n    batch_size=32,\n    learning_rate=1e-4,\n    student_temperature=1.0,\n    teacher_temperature=1.0,\n    alpha=0.5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:11:37.491365Z","iopub.execute_input":"2024-12-25T19:11:37.491735Z","iopub.status.idle":"2024-12-25T19:11:40.405281Z","shell.execute_reply.started":"2024-12-25T19:11:37.491704Z","shell.execute_reply":"2024-12-25T19:11:40.404176Z"}},"outputs":[{"name":"stdout","text":"Train size: 4819, Validation size: 1204\nClass names: ['airplane', 'bear', 'bicycle', 'bird', 'boat', 'bottle', 'car', 'cat', 'chair', 'clock', 'dog', 'elephant', 'keyboard', 'knife', 'oven', 'truck']\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n<ipython-input-2-4ea88560b2cd>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-689287d39a9d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mteacher_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"VGG16\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mteacher_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/working/FinetunedModelsCC/vgg16model_fine_tuned_16_classes.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mteacher_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_fine_tuned_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mstudent_model_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSmallViTModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-4ea88560b2cd>\u001b[0m in \u001b[0;36mload_fine_tuned_model\u001b[0;34m(model_name, model_path, num_classes)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unsupported model name: {model_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1095\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m                 return _load(\n\u001b[0m\u001b[1;32m   1098\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1523\u001b[0m     \u001b[0;31m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1455\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverall_storage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstorage_offset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstorage_offset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_untyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m         \u001b[0;31m# swap here if byteswapping is needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbyteorderdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"model_name = \"ViTsmall\"  \nmodel_path = \"/kaggle/working/DistilledModels/VGG16_distilled_model.pth\" \neval_dataset_path1 = \"/kaggle/input/labellledimagenet/test\" \neval_dataset_path2 = \"/kaggle/input/cue-conflict-splitdata/test\" \nmodel = load_fine_tuned_model(model_name=model_name, model_path=model_path, num_classes=16)\nevaluate_model(model, dataset_path=eval_dataset_path1, batch_size=32)\nevaluate_cue_conflict_dataset(model, eval_dataset_path2, batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:21:04.126209Z","iopub.execute_input":"2024-12-24T17:21:04.126621Z","iopub.status.idle":"2024-12-24T17:21:12.577270Z","shell.execute_reply.started":"2024-12-24T17:21:04.126591Z","shell.execute_reply":"2024-12-24T17:21:12.576168Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-27-4ea88560b2cd>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"ViTsmall model loaded and ready for evaluation.\nEvaluating on dataset with 1513 samples and classes: ['airplane', 'bear', 'bicycle', 'bird', 'boat', 'bottle', 'car', 'cat', 'chair', 'clock', 'dog', 'elephant', 'keyboard', 'knife', 'oven', 'truck']\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 48/48 [00:05<00:00,  8.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on the evaluation dataset: 53.0734 %\nEvaluating on dataset with 256 samples and classes: ['airplane', 'bear', 'bicycle', 'bird', 'boat', 'bottle', 'car', 'cat', 'chair', 'clock', 'dog', 'elephant', 'keyboard', 'knife', 'oven', 'truck']\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.32it/s]","output_type":"stream"},{"name":"stdout","text":"\nFinal Results:\n  Shape Accuracy: 0.0586\n  Texture Accuracy: 0.0664\n  Cue Accuracy: 0.1250\n  Shape Bias: 0.4688\n  Texture Bias: 0.5312\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/labellledimagenet/train\"\nbatch_size = 128\n\ntrain_loader, val_loader = create_data_loaders(dataset_path, batch_size)\nteacher_model_name = \"ViT\"  \nteacher_model_path = \"/kaggle/working/FinetunedModels/vitmodel_fine_tuned_16_classes.pth\"  \nteacher_model = load_fine_tuned_model(teacher_model_name, teacher_model_path, num_classes=16)\nstudent_model_class = SmallViTModel\n\ndistilled_model = distill(\n    model_class=student_model_class, \n    teacher_model=teacher_model, \n    teacher_model_name=teacher_model_name,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    num_classes=16,\n    epochs=8,\n    batch_size=32,\n    learning_rate=1e-4,\n    student_temperature=1.0,\n    teacher_temperature=1.0,\n    alpha=0.5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:22:07.011359Z","iopub.execute_input":"2024-12-24T17:22:07.011722Z","iopub.status.idle":"2024-12-24T17:27:55.392068Z","shell.execute_reply.started":"2024-12-24T17:22:07.011694Z","shell.execute_reply":"2024-12-24T17:27:55.391134Z"}},"outputs":[{"name":"stdout","text":"Train size: 4819, Validation size: 1204\nClass names: ['airplane', 'bear', 'bicycle', 'bird', 'boat', 'bottle', 'car', 'cat', 'chair', 'clock', 'dog', 'elephant', 'keyboard', 'knife', 'oven', 'truck']\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-27-4ea88560b2cd>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"ViT model loaded and ready for evaluation.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/8: 100%|██████████| 38/38 [00:37<00:00,  1.02batch/s, loss=1.45] ","output_type":"stream"},{"name":"stdout","text":"Epoch 1/8, Loss: 1.4545410024492365\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy at epoch 1: 40.37%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/8: 100%|██████████| 38/38 [00:37<00:00,  1.01batch/s, loss=1.25] ","output_type":"stream"},{"name":"stdout","text":"Epoch 2/8, Loss: 1.2458825864289935\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy at epoch 2: 58.55%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/8: 100%|██████████| 38/38 [00:37<00:00,  1.01batch/s, loss=1.18] ","output_type":"stream"},{"name":"stdout","text":"Epoch 3/8, Loss: 1.1766453542207416\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy at epoch 3: 58.97%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/8: 100%|██████████| 38/38 [00:37<00:00,  1.01batch/s, loss=1.16] ","output_type":"stream"},{"name":"stdout","text":"Epoch 4/8, Loss: 1.1560211840428805\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy at epoch 4: 58.97%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/8: 100%|██████████| 38/38 [00:37<00:00,  1.01batch/s, loss=1.15] ","output_type":"stream"},{"name":"stdout","text":"Epoch 5/8, Loss: 1.1489908005061902\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy at epoch 5: 59.05%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/8: 100%|██████████| 38/38 [00:37<00:00,  1.01batch/s, loss=1.14] ","output_type":"stream"},{"name":"stdout","text":"Epoch 6/8, Loss: 1.1437419935276634\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy at epoch 6: 59.05%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/8: 100%|██████████| 38/38 [00:37<00:00,  1.01batch/s, loss=1.14] ","output_type":"stream"},{"name":"stdout","text":"Epoch 7/8, Loss: 1.1400173212352551\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy at epoch 7: 59.05%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/8: 100%|██████████| 38/38 [00:37<00:00,  1.01batch/s, loss=1.14] ","output_type":"stream"},{"name":"stdout","text":"Epoch 8/8, Loss: 1.1377655236344588\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy at epoch 8: 59.05%\nDistilled model saved to /kaggle/working/DistilledModels/ViT_distilled_model.pth\nDistilled model zip file saved to /kaggle/working/DistilledModels/ViT_distilled_model.zip\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"model_name = \"ViTsmall\"  \nmodel_path = \"/kaggle/working/DistilledModels/ViT_distilled_model.pth\" \neval_dataset_path1 = \"/kaggle/input/labellledimagenet/test\" \neval_dataset_path2 = \"/kaggle/input/cue-conflict-splitdata/test\" \nmodel = load_fine_tuned_model(model_name=model_name, model_path=model_path, num_classes=16)\nevaluate_model(model, dataset_path=eval_dataset_path1, batch_size=32)\nevaluate_cue_conflict_dataset(model, eval_dataset_path2, batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:29:17.412342Z","iopub.execute_input":"2024-12-24T17:29:17.412706Z","iopub.status.idle":"2024-12-24T17:29:25.071615Z","shell.execute_reply.started":"2024-12-24T17:29:17.412681Z","shell.execute_reply":"2024-12-24T17:29:25.070598Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-27-4ea88560b2cd>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"ViTsmall model loaded and ready for evaluation.\nEvaluating on dataset with 1513 samples and classes: ['airplane', 'bear', 'bicycle', 'bird', 'boat', 'bottle', 'car', 'cat', 'chair', 'clock', 'dog', 'elephant', 'keyboard', 'knife', 'oven', 'truck']\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 48/48 [00:05<00:00,  8.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on the evaluation dataset: 57.1051 %\nEvaluating on dataset with 256 samples and classes: ['airplane', 'bear', 'bicycle', 'bird', 'boat', 'bottle', 'car', 'cat', 'chair', 'clock', 'dog', 'elephant', 'keyboard', 'knife', 'oven', 'truck']\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 8/8 [00:00<00:00,  9.01it/s]","output_type":"stream"},{"name":"stdout","text":"\nFinal Results:\n  Shape Accuracy: 0.0586\n  Texture Accuracy: 0.0586\n  Cue Accuracy: 0.1172\n  Shape Bias: 0.5000\n  Texture Bias: 0.5000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"**Weighted Ensembling**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport zipfile\nimport os\n\ndef logit_matching_loss(student_logits, ensembled_teacher_logits, student_temperature=1.0, teacher_temperature=1.0):\n    \"\"\"Improved KL divergence loss with temperature scaling\"\"\"\n    student_scaled = student_logits / student_temperature\n    teacher_scaled = ensembled_teacher_logits / teacher_temperature\n    \n    student_log_probs = F.log_softmax(student_scaled, dim=-1)\n    teacher_probs = F.softmax(teacher_scaled, dim=-1)\n    \n    return F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (student_temperature ** 2)\n\ndef weighted_ensemble_logits(teacher_logits_list, confidence_list):\n    \"\"\"Enhanced ensemble weighting with entropy-based confidence\"\"\"\n    stacked_logits = torch.stack(teacher_logits_list, dim=0)\n    \n    # Calculate entropy-based confidence\n    probs = [F.softmax(logits, dim=-1) for logits in teacher_logits_list]\n    entropies = [-torch.sum(p * torch.log(p + 1e-10), dim=-1) for p in probs]\n    confidence_scores = [1.0 / (entropy + 1e-10) for entropy in entropies]\n    \n    # Normalize confidence scores\n    stacked_conf = torch.stack(confidence_scores, dim=0).unsqueeze(-1)\n    normalized_conf = (stacked_conf - stacked_conf.mean()) / (stacked_conf.std() + 1e-6)\n    weights = F.softmax(normalized_conf * 1.5, dim=0)\n    \n    return (stacked_logits * weights).sum(dim=0)\n\ndef distill_with_confidence_aware_ensemble(model_class, teacher_models, teacher_model_names, train_loader, val_loader, \n                                         num_classes, epochs=10, batch_size=32, learning_rate=0.001, \n                                         student_temperature=4.0, teacher_temperature=4.0, alpha=0.7,\n                                         save_dir='/kaggle/working/'):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    student_model = model_class(num_classes=num_classes).to(device)\n    teacher_models = [model.to(device).eval() for model in teacher_models]\n    \n    # Improved optimizer and learning rate scheduler\n    optimizer = optim.AdamW(student_model.parameters(), lr=learning_rate, weight_decay=0.01)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    criterion = nn.CrossEntropyLoss()\n\n    best_val_acc = 0\n    for epoch in range(epochs):\n        student_model.train()\n        running_loss = 0.0\n        \n        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n            for batch_idx, (images, labels) in enumerate(pbar):\n                images, labels = images.to(device), labels.to(device)\n                \n                student_outputs = student_model(images)\n                teacher_logits_list = []\n                confidence_list = []\n                \n                with torch.no_grad():\n                    for teacher in teacher_models:\n                        logits = teacher(images)\n                        probs = F.softmax(logits, dim=-1)\n                        confidence = probs.max(dim=-1)[0]\n                        teacher_logits_list.append(logits)\n                        confidence_list.append(confidence)\n\n                ensemble_logits = weighted_ensemble_logits(teacher_logits_list, confidence_list)\n                \n                ce_loss = criterion(student_outputs, labels)\n                distill_loss = logit_matching_loss(student_outputs, ensemble_logits,\n                                                 student_temperature, teacher_temperature)\n                \n                # Dynamic loss weighting\n                loss = (1 - alpha) * ce_loss + alpha * distill_loss\n                \n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n                optimizer.step()\n                \n                running_loss += loss.item()\n                pbar.set_postfix(loss=running_loss/(batch_idx + 1))\n\n        scheduler.step()\n        \n        # Validation\n        student_model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = student_model(images)\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n                val_loss += criterion(outputs, labels).item()\n\n        accuracy = 100.0 * correct / total\n        print(f\"Epoch {epoch+1}: Train Loss={running_loss/len(train_loader):.4f}, \"\n              f\"Val Loss={val_loss/len(val_loader):.4f}, Val Acc={accuracy:.2f}%\")\n        \n        # Save best model\n        if accuracy > best_val_acc:\n            best_val_acc = accuracy\n            model_path = os.path.join(save_dir, f\"{'+'.join(teacher_model_names)}_distilled_best.pth\")\n            torch.save(student_model.state_dict(), model_path)\n    \n    return student_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:11:45.908711Z","iopub.execute_input":"2024-12-25T19:11:45.909014Z","iopub.status.idle":"2024-12-25T19:11:45.922887Z","shell.execute_reply.started":"2024-12-25T19:11:45.908991Z","shell.execute_reply":"2024-12-25T19:11:45.921952Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Example usage\nteacher_model1 = load_fine_tuned_model('VGG16', '/kaggle/working/FinetunedModelsCC/vgg16model_fine_tuned_16_classes.pth', num_classes=16)\nteacher_model2 = load_fine_tuned_model('ViT', '/kaggle/working/FinetunedModels/vitmodel_fine_tuned_16_classes.pth', num_classes=16)\n\nteacher_models = [teacher_model1, teacher_model2]\nteacher_model_names = ['VGG16', 'ViT']\n\n# Create data loaders\ntrain_loader, val_loader = create_data_loaders('/kaggle/input/labellledimagenet/train', batch_size=32)\n\n# Call the distillation function with multiple teachers\ndistilled_model = distill_with_confidence_aware_ensemble(\n    model_class=SmallViTModel,  # Student model\n    teacher_models=teacher_models,\n    teacher_model_names=teacher_model_names,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    num_classes=16,\n    epochs=8,\n    batch_size=32,\n    learning_rate=0.001,\n    student_temperature=2.0,\n    teacher_temperature=2.0,\n    alpha=0.5,\n    save_dir='/kaggle/working/'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:36:50.575830Z","iopub.execute_input":"2024-12-24T17:36:50.576141Z","iopub.status.idle":"2024-12-24T17:44:09.873828Z","shell.execute_reply.started":"2024-12-24T17:36:50.576118Z","shell.execute_reply":"2024-12-24T17:44:09.872900Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-27-4ea88560b2cd>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"VGG16 model loaded and ready for evaluation.\nViT model loaded and ready for evaluation.\nTrain size: 4819, Validation size: 1204\nClass names: ['airplane', 'bear', 'bicycle', 'bird', 'boat', 'bottle', 'car', 'cat', 'chair', 'clock', 'dog', 'elephant', 'keyboard', 'knife', 'oven', 'truck']\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/8: 100%|██████████| 151/151 [00:49<00:00,  3.03it/s, loss=3.99]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=3.9921, Val Loss=1.7168, Val Acc=58.14%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/8: 100%|██████████| 151/151 [00:49<00:00,  3.04it/s, loss=3.66]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=3.6572, Val Loss=1.7391, Val Acc=57.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/8: 100%|██████████| 151/151 [00:49<00:00,  3.03it/s, loss=3.59]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=3.5928, Val Loss=1.7189, Val Acc=58.72%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/8: 100%|██████████| 151/151 [00:49<00:00,  3.04it/s, loss=3.55]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=3.5542, Val Loss=1.7016, Val Acc=58.22%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/8: 100%|██████████| 151/151 [00:49<00:00,  3.04it/s, loss=3.52]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=3.5241, Val Loss=1.6905, Val Acc=57.89%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/8: 100%|██████████| 151/151 [00:49<00:00,  3.04it/s, loss=3.5] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=3.5005, Val Loss=1.6965, Val Acc=58.39%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/8: 100%|██████████| 151/151 [00:49<00:00,  3.04it/s, loss=3.48]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=3.4809, Val Loss=1.6863, Val Acc=58.39%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/8: 100%|██████████| 151/151 [00:49<00:00,  3.04it/s, loss=3.47]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=3.4688, Val Loss=1.6827, Val Acc=58.39%\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"model_name = \"ViTsmall\"  \nmodel_path = \"/kaggle/working/VGG16+ViT_distilled_best.pth\" \neval_dataset_path1 = \"/kaggle/input/labellledimagenet/test\" \neval_dataset_path2 = \"/kaggle/input/cue-conflict-splitdata/test\" \nmodel = load_fine_tuned_model(model_name=model_name, model_path=model_path, num_classes=16)\nevaluate_model(model, dataset_path=eval_dataset_path1, batch_size=32)\nevaluate_cue_conflict_dataset(model, eval_dataset_path2, batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:45:58.748697Z","iopub.execute_input":"2024-12-24T17:45:58.749033Z","iopub.status.idle":"2024-12-24T17:46:05.700778Z","shell.execute_reply.started":"2024-12-24T17:45:58.749007Z","shell.execute_reply":"2024-12-24T17:46:05.699774Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-27-4ea88560b2cd>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"ViTsmall model loaded and ready for evaluation.\nEvaluating on dataset with 1513 samples and classes: ['airplane', 'bear', 'bicycle', 'bird', 'boat', 'bottle', 'car', 'cat', 'chair', 'clock', 'dog', 'elephant', 'keyboard', 'knife', 'oven', 'truck']\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 48/48 [00:05<00:00,  8.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on the evaluation dataset: 55.7171 %\nEvaluating on dataset with 256 samples and classes: ['airplane', 'bear', 'bicycle', 'bird', 'boat', 'bottle', 'car', 'cat', 'chair', 'clock', 'dog', 'elephant', 'keyboard', 'knife', 'oven', 'truck']\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 8/8 [00:00<00:00,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"\nFinal Results:\n  Shape Accuracy: 0.0586\n  Texture Accuracy: 0.0586\n  Cue Accuracy: 0.1172\n  Shape Bias: 0.5000\n  Texture Bias: 0.5000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"**Evaluation**","metadata":{}},{"cell_type":"code","source":"\ndef evaluate_model(model, dataset_path, batch_size=32):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)), \n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n    \n    eval_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n    eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    print(f\"Evaluating on dataset with {len(eval_dataset)} samples and classes: {eval_dataset.classes}\")\n\n    total_samples = 0\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for images, labels in tqdm(eval_loader, desc=\"Evaluating\"):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            \n            correct_predictions += torch.sum(preds == labels).item()\n            total_samples += labels.size(0)\n    \n    accuracy = (correct_predictions / total_samples) * 100\n    print(f\"Accuracy on the evaluation dataset: {accuracy:.4f} %\")\n    \n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:35:27.396645Z","iopub.execute_input":"2024-12-25T17:35:27.396953Z","iopub.status.idle":"2024-12-25T17:35:27.403282Z","shell.execute_reply.started":"2024-12-25T17:35:27.396928Z","shell.execute_reply":"2024-12-25T17:35:27.402269Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport re\n\ndef strip_label_suffix(label):\n    return re.sub(r'\\d+$', '', label)\n\ndef evaluate_cue_conflict_dataset(model, dataset_path, batch_size=32):\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    eval_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n    eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    print(f\"Evaluating on dataset with {len(eval_dataset)} samples and classes: {eval_dataset.classes}\")\n\n    shape_correct = 0\n    texture_correct = 0\n    total_samples = 0\n\n    cue_correct = 0\n    \n    with torch.no_grad():\n        for images, labels in tqdm(eval_loader, desc=\"Evaluating\"):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n    \n            for i, pred in enumerate(preds):\n                filename = eval_dataset.samples[total_samples + i][0]\n                shape_texture = os.path.basename(filename).split(\"-\")\n                shape_label = strip_label_suffix(shape_texture[0]) \n                texture_label = strip_label_suffix(shape_texture[1][:-4])  \n                pred_class = eval_dataset.classes[pred]\n    \n                if pred_class == shape_label:\n                    shape_correct += 1\n                    cue_correct += 1  # Count once for shape match\n                elif pred_class == texture_label:  # Use elif to avoid double-counting\n                    texture_correct += 1\n                    cue_correct += 1\n            total_samples += len(preds)\n    \n    shape_accuracy = shape_correct / total_samples if total_samples > 0 else 0\n    texture_accuracy = texture_correct / total_samples if total_samples > 0 else 0\n    cue_accuracy = cue_correct / total_samples if total_samples > 0 else 0\n    shape_bias = shape_accuracy / cue_accuracy if cue_accuracy > 0 else 0\n    texture_bias = 1 - shape_bias\n\n\n    print(\"\\nFinal Results:\")\n    print(f\"  Shape Accuracy: {shape_accuracy:.4f}\")\n    print(f\"  Texture Accuracy: {texture_accuracy:.4f}\")\n    print(f\"  Cue Accuracy: {cue_accuracy:.4f}\")\n    print(f\"  Shape Bias: {shape_bias:.4f}\")\n    print(f\"  Texture Bias: {texture_bias:.4f}\")\n\n    # return {\n    #     \"Shape Accuracy\": shape_accuracy,\n    #     \"Texture Accuracy\": texture_accuracy,\n    #     \"Cue Accuracy\": cue_accuracy,\n    #     \"Shape Bias\": shape_bias,\n    #     \"Texture Bias\": texture_bias\n    # }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:35:30.826864Z","iopub.execute_input":"2024-12-25T17:35:30.827170Z","iopub.status.idle":"2024-12-25T17:35:30.836004Z","shell.execute_reply.started":"2024-12-25T17:35:30.827142Z","shell.execute_reply":"2024-12-25T17:35:30.835034Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"model_name = \"ViT\"  \nmodel_path = \"/kaggle/working/FinetunedModels/vitmodel_fine_tuned_16_classes.pth\" \n# eval_dataset_path1 = \"/kaggle/input/labelledimagenet\" \neval_dataset_path2 = \"/kaggle/input/cue-conflict-splitdata/test\" \nmodel = load_fine_tuned_model(model_name=model_name, model_path=model_path, num_classes=16)\n# evaluate_model(model, dataset_path=eval_dataset_path1, batch_size=32)\nevaluate_cue_conflict_dataset(model, eval_dataset_path2, batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T14:33:20.036773Z","iopub.status.idle":"2024-12-24T14:33:20.037028Z","shell.execute_reply":"2024-12-24T14:33:20.036912Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Distilling Intermediate Loss**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport timm\nfrom torchvision import models\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self, model, layers_to_extract):\n        super(FeatureExtractor, self).__init__()\n        self.model = model\n        self.layers_to_extract = layers_to_extract\n        \n    def forward(self, x):\n        features = []\n        for name, module in self.model.named_children():\n            try:\n                x = module(x)\n                if name in self.layers_to_extract:\n                    print(f\"Extracted features from layer {name}, shape: {x.shape}\")\n                    features.append(x)\n            except Exception as e:\n                print(f\"Error in layer {name}: {str(e)}\")\n                raise\n        return features\n\nclass VGG16FeatureExtractor(FeatureExtractor):\n    def __init__(self, model, layers_to_extract=['features']):\n        super(VGG16FeatureExtractor, self).__init__(model, layers_to_extract)\n        \nclass ViTFeatureExtractor(FeatureExtractor):\n    def __init__(self, model, layers_to_extract=['blocks']):\n        super(ViTFeatureExtractor, self).__init__(model, layers_to_extract)\n\ndef intermediate_distillation_loss(student_features, teacher_features, alpha=0.5, temperature=2.0):\n    total_loss = 0.0\n    for idx, (student_feature, teacher_feature) in enumerate(zip(student_features, teacher_features)):\n        try:\n            print(f\"\\nFeature pair {idx}:\")\n            print(f\"Student feature shape: {student_feature.shape}\")\n            print(f\"Teacher feature shape: {teacher_feature.shape}\")\n            \n            student_feature = student_feature / student_feature.norm(dim=1, keepdim=True)\n            teacher_feature = teacher_feature / teacher_feature.norm(dim=1, keepdim=True)\n            \n            if student_feature.shape != teacher_feature.shape:\n                print(f\"Resizing teacher features from {teacher_feature.shape} to match student shape {student_feature.shape}\")\n                if len(student_feature.shape) == 4:  \n                    teacher_feature = nn.functional.interpolate(\n                        teacher_feature,\n                        size=student_feature.shape[2:],\n                        mode='bilinear',\n                        align_corners=False\n                    )\n                elif len(student_feature.shape) == 3: \n                    teacher_feature = teacher_feature[:, :student_feature.shape[1], :]\n                    if teacher_feature.shape[-1] != student_feature.shape[-1]:\n                        projection = nn.Linear(teacher_feature.shape[-1], student_feature.shape[-1]).to(teacher_feature.device)\n                        teacher_feature = projection(teacher_feature)\n            \n            student_feature = student_feature / temperature\n            teacher_feature = teacher_feature / temperature\n            distill_loss = nn.functional.mse_loss(student_feature, teacher_feature)\n            total_loss += distill_loss * alpha\n            print(f\"Distillation loss for feature pair {idx}: {distill_loss.item()}\")\n            \n        except Exception as e:\n            print(f\"Error in distillation loss calculation for feature pair {idx}: {str(e)}\")\n            raise\n            \n    return total_loss\n\ndef intermediate_distillation_loss(student_features, teacher_features, alpha=0.5, temperature=2.0):\n    total_loss = torch.tensor(0.0).to(student_features[0].device)  \n    \n    for idx, (student_feature, teacher_feature) in enumerate(zip(student_features, teacher_features)):\n        try:\n            print(f\"\\nFeature pair {idx}:\")\n            print(f\"Student feature shape: {student_feature.shape}\")\n            print(f\"Teacher feature shape: {teacher_feature.shape}\")\n            if not isinstance(student_feature, torch.Tensor):\n                print(f\"Warning: Converting student feature to tensor\")\n                student_feature = torch.tensor(student_feature)\n            if not isinstance(teacher_feature, torch.Tensor):\n                print(f\"Warning: Converting teacher feature to tensor\")\n                teacher_feature = torch.tensor(teacher_feature)\n            student_feature = student_feature.to(total_loss.device)\n            teacher_feature = teacher_feature.to(total_loss.device)\n            \n            # student_feature = student_feature / (student_feature.norm(dim=1, keepdim=True) + 1e-6)\n            # teacher_feature = teacher_feature / (teacher_feature.norm(dim=1, keepdim=True) + 1e-6)\n            \n            if student_feature.shape != teacher_feature.shape:\n                print(f\"Resizing teacher features from {teacher_feature.shape} to match student shape {student_feature.shape}\")\n                if len(student_feature.shape) == 4: \n                    teacher_feature = nn.functional.interpolate(\n                        teacher_feature,\n                        size=student_feature.shape[2:],\n                        mode='bilinear',\n                        align_corners=False\n                    )\n                elif len(student_feature.shape) == 3: \n                    teacher_feature = teacher_feature[:, :student_feature.shape[1], :]\n                    if teacher_feature.shape[-1] != student_feature.shape[-1]:\n                        projection = nn.Linear(teacher_feature.shape[-1], student_feature.shape[-1]).to(teacher_feature.device)\n                        teacher_feature = projection(teacher_feature)\n            \n            student_feature = student_feature / temperature\n            teacher_feature = teacher_feature / temperature\n            \n            distill_loss = nn.functional.mse_loss(student_feature, teacher_feature)\n            total_loss += distill_loss * alpha\n            print(f\"Distillation loss for feature pair {idx}: {distill_loss.item()}\")\n            \n        except Exception as e:\n            print(f\"Error in distillation loss calculation for feature pair {idx}: {str(e)}\")\n            print(f\"Student feature type: {type(student_feature)}\")\n            print(f\"Teacher feature type: {type(teacher_feature)}\")\n            raise\n            \n    return total_loss\n\ndef compute_accuracy(output, labels):\n    _, predicted = torch.max(output, 1)\n    correct = (predicted == labels).sum().item()  \n    accuracy = correct / labels.size(0)  \n    return accuracy\n\ndef distill_with_intermediate_distillation(student_model, teacher_models, train_loader, val_loader, epochs=10, \n                                           learning_rate=0.001, alpha=0.5, temperature=2.0, device='cuda'):\n    print(f\"\\nStarting distillation process:\")\n    print(f\"Device: {device}\")\n    print(f\"Learning rate: {learning_rate}\")\n    print(f\"Alpha: {alpha}\")\n    print(f\"Temperature: {temperature}\")\n    \n    student_model.to(device)\n    \n    optimizer = torch.optim.Adam(student_model.parameters(), lr=learning_rate)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n    \n    teacher_extractors = []\n    for idx, teacher_model in enumerate(teacher_models):\n        print(f\"\\nInitializing teacher model {idx + 1}\")\n        teacher_model.eval()\n        teacher_model.to(device)\n        \n        if isinstance(teacher_model, models.vgg.VGG):\n            extractor = VGG16FeatureExtractor(teacher_model)\n            print(\"Using VGG16 feature extractor\")\n        elif 'ViT' in str(type(teacher_model)):\n            extractor = ViTFeatureExtractor(teacher_model)\n            print(\"Using ViT feature extractor\")\n        else:\n            extractor = FeatureExtractor(teacher_model, ['features', 'blocks'])\n            print(\"Using default feature extractor\")\n            \n        teacher_extractors.append(extractor)\n    \n    student_model.train()\n    best_val_loss = float('inf')\n    \n    for epoch in range(epochs):\n        print(f\"\\nEpoch [{epoch + 1}/{epochs}]\")\n        total_train_loss = torch.tensor(0.0).to(device)  # Initialize as tensor\n        total_train_accuracy = 0.0\n        num_batches = 0\n        \n        for batch_idx, (inputs, labels) in enumerate(train_loader):\n            try:\n                inputs = inputs.to(device)\n                labels = labels.to(device).long()\n                \n                print(f\"\\nBatch {batch_idx + 1}:\")\n                print(f\"Input shape: {inputs.shape}\")\n                print(f\"Labels shape: {labels.shape}\")\n                optimizer.zero_grad()\n                \n                student_features = student_model(inputs)\n                if not isinstance(student_features, list):\n                    student_features = [student_features]\n                \n                teacher_features_list = []\n                for teacher_idx, extractor in enumerate(teacher_extractors):\n                    print(f\"\\nExtracting features from teacher {teacher_idx + 1}\")\n                    with torch.no_grad():\n                        teacher_features = extractor(inputs)\n                    teacher_features_list.append(teacher_features)\n                \n                distill_loss = torch.tensor(0.0).to(device)  # Initialize as tensor\n                for teacher_idx, teacher_features in enumerate(teacher_features_list):\n                    print(f\"\\nComputing distillation loss for teacher {teacher_idx + 1}\")\n                    current_distill_loss = intermediate_distillation_loss(\n                        student_features, \n                        teacher_features, \n                        alpha, \n                        temperature\n                    )\n                    distill_loss += current_distill_loss\n                \n                student_output = student_features[-1]\n                classification_loss = nn.CrossEntropyLoss()(student_output, labels)\n                \n                total_loss = distill_loss + classification_loss\n                print(f\"\\nLosses for batch {batch_idx + 1}:\")\n                print(f\"Distillation loss: {distill_loss.item():.4f}\")\n                print(f\"Classification loss: {classification_loss.item():.4f}\")\n                print(f\"Total loss: {total_loss.item():.4f}\")\n                \n                total_loss.backward()\n                optimizer.step()\n                \n                total_train_loss += total_loss\n                total_train_accuracy += compute_accuracy(student_output, labels)\n                num_batches += 1\n                \n            except Exception as e:\n                print(f\"Error in batch {batch_idx + 1}: {str(e)}\")\n                continue\n        \n        avg_train_loss = (total_train_loss / num_batches).item()\n        avg_train_accuracy = (total_train_accuracy / num_batches) * 100  \n        print(f\"\\nEpoch {epoch + 1} average training loss: {avg_train_loss:.4f}\")\n        print(f\"Epoch {epoch + 1} average training accuracy: {avg_train_accuracy:.2f}%\")\n        \n        student_model.eval()\n        val_loss = 0.0\n        val_accuracy = 0.0\n        num_val_batches = 0\n        \n        with torch.no_grad():\n            for val_inputs, val_labels in val_loader:\n                try:\n                    val_inputs = val_inputs.to(device)\n                    val_labels = val_labels.to(device).long()\n                    \n                    val_student_features = student_model(val_inputs)\n                    if not isinstance(val_student_features, list):\n                        val_student_features = [val_student_features]\n                    \n                    val_distill_loss = 0.0\n                    for extractor in teacher_extractors:\n                        teacher_features = extractor(val_inputs)\n                        val_distill_loss += intermediate_distillation_loss(\n                            val_student_features,\n                            teacher_features,\n                            alpha,\n                            temperature\n                        )\n                    \n                    val_classification_loss = nn.CrossEntropyLoss()(val_student_features[-1], val_labels)\n                    val_total_loss = val_distill_loss + val_classification_loss\n                    val_loss += val_total_loss.item()\n                    \n                    val_accuracy += compute_accuracy(val_student_features[-1], val_labels)\n                    num_val_batches += 1\n                    \n                except Exception as e:\n                    print(f\"Error in validation batch: {str(e)}\")\n                    continue\n        \n        avg_val_loss = val_loss / num_val_batches\n        avg_val_accuracy = (val_accuracy / num_val_batches) * 100  # Percentage\n        print(f\"Validation loss: {avg_val_loss:.4f}\")\n        print(f\"Validation accuracy: {avg_val_accuracy:.2f}%\")\n        scheduler.step(avg_val_loss)\n        \n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save(student_model.state_dict(), 'best_student_model.pth')\n            print(\"Saved best model checkpoint\")\n        \n        student_model.train()\n    \n    return student_model\n\n\ntrain_loader, val_loader = create_data_loaders('/kaggle/input/labellledimagenet/train', batch_size=16)\nteacher_model1 = load_fine_tuned_model('VGG16', '/kaggle/working/FinetunedModelsCC/vgg16model_fine_tuned_16_classes.pth', num_classes=16)\nteacher_model2 = load_fine_tuned_model('ViT', '/kaggle/working/FinetunedModels/vitmodel_fine_tuned_16_classes.pth', num_classes=16)\nstudent_model = SmallViTModel(num_classes=16, pretrained=False)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndistilled_model = distill_with_intermediate_distillation(\n    student_model=student_model,\n    teacher_models=[teacher_model1, teacher_model2],\n    train_loader=train_loader,\n    val_loader=val_loader,\n    epochs=8,\n    learning_rate=0.001,\n    alpha=0.5,\n    temperature=2.0,\n    device=device \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:24:59.103265Z","iopub.execute_input":"2024-12-25T19:24:59.103565Z","iopub.status.idle":"2024-12-25T19:25:31.410916Z","shell.execute_reply.started":"2024-12-25T19:24:59.103545Z","shell.execute_reply":"2024-12-25T19:25:31.409688Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Train size: 4819, Validation size: 1204\nClass names: ['airplane', 'bear', 'bicycle', 'bird', 'boat', 'bottle', 'car', 'cat', 'chair', 'clock', 'dog', 'elephant', 'keyboard', 'knife', 'oven', 'truck']\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-2-4ea88560b2cd>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"VGG16 model loaded and ready for evaluation.\nViT model loaded and ready for evaluation.\n\nStarting distillation process:\nDevice: cuda\nLearning rate: 0.001\nAlpha: 0.5\nTemperature: 2.0\n\nInitializing teacher model 1\nUsing default feature extractor\n\nInitializing teacher model 2\nUsing ViT feature extractor\n\nEpoch [1/8]\n\nBatch 1:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 1:\nDistillation loss: 0.0000\nClassification loss: 3.0366\nTotal loss: 3.0366\n\nBatch 2:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 2:\nDistillation loss: 0.0000\nClassification loss: 3.1179\nTotal loss: 3.1179\n\nBatch 3:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 3:\nDistillation loss: 0.0000\nClassification loss: 7.0365\nTotal loss: 7.0365\n\nBatch 4:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 4:\nDistillation loss: 0.0000\nClassification loss: 3.0925\nTotal loss: 3.0925\n\nBatch 5:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 5:\nDistillation loss: 0.0000\nClassification loss: 2.9391\nTotal loss: 2.9391\n\nBatch 6:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 6:\nDistillation loss: 0.0000\nClassification loss: 2.3767\nTotal loss: 2.3767\n\nBatch 7:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 7:\nDistillation loss: 0.0000\nClassification loss: 2.8101\nTotal loss: 2.8101\n\nBatch 8:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 8:\nDistillation loss: 0.0000\nClassification loss: 1.7655\nTotal loss: 1.7655\n\nBatch 9:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 9:\nDistillation loss: 0.0000\nClassification loss: 2.0144\nTotal loss: 2.0144\n\nBatch 10:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 10:\nDistillation loss: 0.0000\nClassification loss: 1.8661\nTotal loss: 1.8661\n\nBatch 11:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 11:\nDistillation loss: 0.0000\nClassification loss: 1.3858\nTotal loss: 1.3858\n\nBatch 12:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 12:\nDistillation loss: 0.0000\nClassification loss: 1.2641\nTotal loss: 1.2641\n\nBatch 13:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 13:\nDistillation loss: 0.0000\nClassification loss: 1.2744\nTotal loss: 1.2744\n\nBatch 14:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 14:\nDistillation loss: 0.0000\nClassification loss: 2.5524\nTotal loss: 2.5524\n\nBatch 15:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 15:\nDistillation loss: 0.0000\nClassification loss: 2.4292\nTotal loss: 2.4292\n\nBatch 16:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 16:\nDistillation loss: 0.0000\nClassification loss: 1.8210\nTotal loss: 1.8210\n\nBatch 17:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 17:\nDistillation loss: 0.0000\nClassification loss: 1.4989\nTotal loss: 1.4989\n\nBatch 18:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 18:\nDistillation loss: 0.0000\nClassification loss: 1.1849\nTotal loss: 1.1849\n\nBatch 19:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 19:\nDistillation loss: 0.0000\nClassification loss: 1.9469\nTotal loss: 1.9469\n\nBatch 20:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 20:\nDistillation loss: 0.0000\nClassification loss: 1.6955\nTotal loss: 1.6955\n\nBatch 21:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 21:\nDistillation loss: 0.0000\nClassification loss: 1.9062\nTotal loss: 1.9062\n\nBatch 22:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 22:\nDistillation loss: 0.0000\nClassification loss: 1.3944\nTotal loss: 1.3944\n\nBatch 23:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 23:\nDistillation loss: 0.0000\nClassification loss: 1.9844\nTotal loss: 1.9844\n\nBatch 24:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 24:\nDistillation loss: 0.0000\nClassification loss: 1.7591\nTotal loss: 1.7591\n\nBatch 25:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 25:\nDistillation loss: 0.0000\nClassification loss: 1.5992\nTotal loss: 1.5992\n\nBatch 26:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 26:\nDistillation loss: 0.0000\nClassification loss: 1.4444\nTotal loss: 1.4444\n\nBatch 27:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 27:\nDistillation loss: 0.0000\nClassification loss: 1.0287\nTotal loss: 1.0287\n\nBatch 28:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 28:\nDistillation loss: 0.0000\nClassification loss: 1.1430\nTotal loss: 1.1430\n\nBatch 29:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 29:\nDistillation loss: 0.0000\nClassification loss: 2.2147\nTotal loss: 2.2147\n\nBatch 30:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 30:\nDistillation loss: 0.0000\nClassification loss: 1.4491\nTotal loss: 1.4491\n\nBatch 31:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 31:\nDistillation loss: 0.0000\nClassification loss: 1.8828\nTotal loss: 1.8828\n\nBatch 32:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 32:\nDistillation loss: 0.0000\nClassification loss: 1.4362\nTotal loss: 1.4362\n\nBatch 33:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 33:\nDistillation loss: 0.0000\nClassification loss: 2.2463\nTotal loss: 2.2463\n\nBatch 34:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 34:\nDistillation loss: 0.0000\nClassification loss: 1.0672\nTotal loss: 1.0672\n\nBatch 35:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 35:\nDistillation loss: 0.0000\nClassification loss: 1.7651\nTotal loss: 1.7651\n\nBatch 36:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 36:\nDistillation loss: 0.0000\nClassification loss: 1.6499\nTotal loss: 1.6499\n\nBatch 37:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 37:\nDistillation loss: 0.0000\nClassification loss: 1.4286\nTotal loss: 1.4286\n\nBatch 38:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 38:\nDistillation loss: 0.0000\nClassification loss: 1.7979\nTotal loss: 1.7979\n\nBatch 39:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 39:\nDistillation loss: 0.0000\nClassification loss: 1.5320\nTotal loss: 1.5320\n\nBatch 40:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 40:\nDistillation loss: 0.0000\nClassification loss: 1.5359\nTotal loss: 1.5359\n\nBatch 41:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 41:\nDistillation loss: 0.0000\nClassification loss: 1.7026\nTotal loss: 1.7026\n\nBatch 42:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 42:\nDistillation loss: 0.0000\nClassification loss: 1.5292\nTotal loss: 1.5292\n\nBatch 43:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 43:\nDistillation loss: 0.0000\nClassification loss: 1.8821\nTotal loss: 1.8821\n\nBatch 44:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 44:\nDistillation loss: 0.0000\nClassification loss: 1.3198\nTotal loss: 1.3198\n\nBatch 45:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 45:\nDistillation loss: 0.0000\nClassification loss: 1.5473\nTotal loss: 1.5473\n\nBatch 46:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 46:\nDistillation loss: 0.0000\nClassification loss: 1.7088\nTotal loss: 1.7088\n\nBatch 47:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 47:\nDistillation loss: 0.0000\nClassification loss: 1.8767\nTotal loss: 1.8767\n\nBatch 48:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 48:\nDistillation loss: 0.0000\nClassification loss: 0.9896\nTotal loss: 0.9896\n\nBatch 49:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 49:\nDistillation loss: 0.0000\nClassification loss: 2.4379\nTotal loss: 2.4379\n\nBatch 50:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 50:\nDistillation loss: 0.0000\nClassification loss: 2.3154\nTotal loss: 2.3154\n\nBatch 51:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 51:\nDistillation loss: 0.0000\nClassification loss: 1.4516\nTotal loss: 1.4516\n\nBatch 52:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 52:\nDistillation loss: 0.0000\nClassification loss: 1.6528\nTotal loss: 1.6528\n\nBatch 53:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 53:\nDistillation loss: 0.0000\nClassification loss: 1.7901\nTotal loss: 1.7901\n\nBatch 54:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 54:\nDistillation loss: 0.0000\nClassification loss: 1.1778\nTotal loss: 1.1778\n\nBatch 55:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 55:\nDistillation loss: 0.0000\nClassification loss: 1.1664\nTotal loss: 1.1664\n\nBatch 56:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 56:\nDistillation loss: 0.0000\nClassification loss: 1.2529\nTotal loss: 1.2529\n\nBatch 57:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 57:\nDistillation loss: 0.0000\nClassification loss: 2.0290\nTotal loss: 2.0290\n\nBatch 58:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 58:\nDistillation loss: 0.0000\nClassification loss: 1.5181\nTotal loss: 1.5181\n\nBatch 59:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 59:\nDistillation loss: 0.0000\nClassification loss: 1.7092\nTotal loss: 1.7092\n\nBatch 60:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 60:\nDistillation loss: 0.0000\nClassification loss: 1.4029\nTotal loss: 1.4029\n\nBatch 61:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 61:\nDistillation loss: 0.0000\nClassification loss: 1.7186\nTotal loss: 1.7186\n\nBatch 62:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 62:\nDistillation loss: 0.0000\nClassification loss: 1.5174\nTotal loss: 1.5174\n\nBatch 63:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 63:\nDistillation loss: 0.0000\nClassification loss: 2.0161\nTotal loss: 2.0161\n\nBatch 64:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 64:\nDistillation loss: 0.0000\nClassification loss: 1.6443\nTotal loss: 1.6443\n\nBatch 65:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 65:\nDistillation loss: 0.0000\nClassification loss: 2.1378\nTotal loss: 2.1378\n\nBatch 66:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 66:\nDistillation loss: 0.0000\nClassification loss: 1.4436\nTotal loss: 1.4436\n\nBatch 67:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 67:\nDistillation loss: 0.0000\nClassification loss: 1.9869\nTotal loss: 1.9869\n\nBatch 68:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 68:\nDistillation loss: 0.0000\nClassification loss: 1.7622\nTotal loss: 1.7622\n\nBatch 69:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 69:\nDistillation loss: 0.0000\nClassification loss: 1.5162\nTotal loss: 1.5162\n\nBatch 70:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 70:\nDistillation loss: 0.0000\nClassification loss: 1.7489\nTotal loss: 1.7489\n\nBatch 71:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 71:\nDistillation loss: 0.0000\nClassification loss: 1.4134\nTotal loss: 1.4134\n\nBatch 72:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 72:\nDistillation loss: 0.0000\nClassification loss: 1.5381\nTotal loss: 1.5381\n\nBatch 73:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 73:\nDistillation loss: 0.0000\nClassification loss: 1.7425\nTotal loss: 1.7425\n\nBatch 74:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 74:\nDistillation loss: 0.0000\nClassification loss: 1.1908\nTotal loss: 1.1908\n\nBatch 75:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 75:\nDistillation loss: 0.0000\nClassification loss: 1.2813\nTotal loss: 1.2813\n\nBatch 76:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 76:\nDistillation loss: 0.0000\nClassification loss: 2.2741\nTotal loss: 2.2741\n\nBatch 77:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 77:\nDistillation loss: 0.0000\nClassification loss: 2.1001\nTotal loss: 2.1001\n\nBatch 78:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 78:\nDistillation loss: 0.0000\nClassification loss: 1.0481\nTotal loss: 1.0481\n\nBatch 79:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 79:\nDistillation loss: 0.0000\nClassification loss: 1.6418\nTotal loss: 1.6418\n\nBatch 80:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 80:\nDistillation loss: 0.0000\nClassification loss: 1.2574\nTotal loss: 1.2574\n\nBatch 81:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 81:\nDistillation loss: 0.0000\nClassification loss: 1.8163\nTotal loss: 1.8163\n\nBatch 82:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 82:\nDistillation loss: 0.0000\nClassification loss: 1.6694\nTotal loss: 1.6694\n\nBatch 83:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 83:\nDistillation loss: 0.0000\nClassification loss: 1.5656\nTotal loss: 1.5656\n\nBatch 84:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 84:\nDistillation loss: 0.0000\nClassification loss: 1.4434\nTotal loss: 1.4434\n\nBatch 85:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 85:\nDistillation loss: 0.0000\nClassification loss: 2.3958\nTotal loss: 2.3958\n\nBatch 86:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 86:\nDistillation loss: 0.0000\nClassification loss: 1.3846\nTotal loss: 1.3846\n\nBatch 87:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 87:\nDistillation loss: 0.0000\nClassification loss: 1.6102\nTotal loss: 1.6102\n\nBatch 88:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 88:\nDistillation loss: 0.0000\nClassification loss: 1.8062\nTotal loss: 1.8062\n\nBatch 89:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 89:\nDistillation loss: 0.0000\nClassification loss: 1.9777\nTotal loss: 1.9777\n\nBatch 90:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 90:\nDistillation loss: 0.0000\nClassification loss: 1.3598\nTotal loss: 1.3598\n\nBatch 91:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 91:\nDistillation loss: 0.0000\nClassification loss: 1.3800\nTotal loss: 1.3800\n\nBatch 92:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 92:\nDistillation loss: 0.0000\nClassification loss: 1.6506\nTotal loss: 1.6506\n\nBatch 93:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 93:\nDistillation loss: 0.0000\nClassification loss: 1.3561\nTotal loss: 1.3561\n\nBatch 94:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 94:\nDistillation loss: 0.0000\nClassification loss: 1.4403\nTotal loss: 1.4403\n\nBatch 95:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 95:\nDistillation loss: 0.0000\nClassification loss: 1.5315\nTotal loss: 1.5315\n\nBatch 96:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 96:\nDistillation loss: 0.0000\nClassification loss: 2.2510\nTotal loss: 2.2510\n\nBatch 97:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 97:\nDistillation loss: 0.0000\nClassification loss: 1.0177\nTotal loss: 1.0177\n\nBatch 98:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 98:\nDistillation loss: 0.0000\nClassification loss: 1.2139\nTotal loss: 1.2139\n\nBatch 99:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 99:\nDistillation loss: 0.0000\nClassification loss: 1.7970\nTotal loss: 1.7970\n\nBatch 100:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 100:\nDistillation loss: 0.0000\nClassification loss: 0.9784\nTotal loss: 0.9784\n\nBatch 101:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 101:\nDistillation loss: 0.0000\nClassification loss: 1.0910\nTotal loss: 1.0910\n\nBatch 102:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 102:\nDistillation loss: 0.0000\nClassification loss: 1.4309\nTotal loss: 1.4309\n\nBatch 103:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 103:\nDistillation loss: 0.0000\nClassification loss: 1.1818\nTotal loss: 1.1818\n\nBatch 104:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 104:\nDistillation loss: 0.0000\nClassification loss: 1.8053\nTotal loss: 1.8053\n\nBatch 105:\nInput shape: torch.Size([16, 3, 224, 224])\nLabels shape: torch.Size([16])\n\nExtracting features from teacher 1\n\nExtracting features from teacher 2\n\nComputing distillation loss for teacher 1\n\nComputing distillation loss for teacher 2\n\nLosses for batch 105:\nDistillation loss: 0.0000\nClassification loss: 1.7040\nTotal loss: 1.7040\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-eee250843b75>\u001b[0m in \u001b[0;36m<cell line: 322>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m distilled_model = distill_with_intermediate_distillation(\n\u001b[0m\u001b[1;32m    323\u001b[0m     \u001b[0mstudent_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudent_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0mteacher_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mteacher_model1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_model2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-eee250843b75>\u001b[0m in \u001b[0;36mdistill_with_intermediate_distillation\u001b[0;34m(student_model, teacher_models, train_loader, val_loader, epochs, learning_rate, alpha, temperature, device)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                 \u001b[0mtotal_train_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m                 \u001b[0mnum_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-eee250843b75>\u001b[0m in \u001b[0;36mcompute_accuracy\u001b[0;34m(output, labels)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# Get the predicted class indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Count the correct predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute the accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":28}]}